{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\"\n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "# Week 1 - End-to-end Deployment using Hugging Face and Sagemaker\n",
    "\n",
    "### 🛍️ Hugging Face and Sagemaker\n",
    "\n",
    "Hugging Face is the creator of Transformers, the leading open-source library for building state-of-the-art machine learning models. Hugging Face allows you to choose from tens of thousands of machine learning models for Natural Language Processing, Audio, and Computer Vision, publicly available in the Hugging Face Hub, to accelerate your machine learning workload.\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. You can deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.\n",
    "\n",
    "One Command is All you Need!\n",
    "\n",
    "With the new Hugging Face Deep Learning Containers available in Amazon SageMaker, training cutting-edge Transformers-based NLP models is much easier. There are variants specially optimized for TensorFlow and PyTorch, for single-GPU, single-node multi-GPU and multi-node clusters.\n",
    "\n",
    "In this session, you will learn how to use Amazon SageMaker to train a Hugging Face Transformer model and deploy it afterwards.\n",
    "\n",
    "\n",
    "### 📚 Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Prepare and upload a test dataset to AWS S3\n",
    "- Prepare a fine-tuning script to be used with Amazon SageMaker Training jobs\n",
    "- Launch a training job and store the trained model into S3\n",
    "- Deploy the model after successful training\n",
    "\n",
    "### 📝 Note.\n",
    "In this session, not all the imports are provided; you may need to import necessary modules / functions to be able to run the code successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (4.22.0)\n",
      "Requirement already satisfied: torch in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied: filelock in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.109.0.tar.gz (571 kB)\n",
      "\u001b[K     |████████████████████████████████| 571 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (1.23.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (2022.9.13)\n",
      "Requirement already satisfied: requests in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (4.64.1)\n",
      "Requirement already satisfied: filelock in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from transformers==4.6.1) (3.8.0)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=1.0.0<4.0.0\n",
      "  Downloading pyarrow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.3 MB 334 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from datasets[s3]==1.6.2) (1.4.3)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.12)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.2)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from sagemaker>=2.48.0) (21.4.0)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.108.0.tar.gz (570 kB)\n",
      "\u001b[K     |████████████████████████████████| 570 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.107.0.tar.gz (568 kB)\n",
      "\u001b[K     |████████████████████████████████| 568 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.106.0.tar.gz (568 kB)\n",
      "\u001b[K     |████████████████████████████████| 568 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.105.0.tar.gz (567 kB)\n",
      "\u001b[K     |████████████████████████████████| 567 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.104.0.tar.gz (566 kB)\n",
      "\u001b[K     |████████████████████████████████| 566 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.103.0.tar.gz (555 kB)\n",
      "\u001b[K     |████████████████████████████████| 555 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.102.0.tar.gz (555 kB)\n",
      "\u001b[K     |████████████████████████████████| 555 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.101.1.tar.gz (552 kB)\n",
      "\u001b[K     |████████████████████████████████| 552 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.100.0.tar.gz (545 kB)\n",
      "\u001b[K     |████████████████████████████████| 545 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.99.0.tar.gz (542 kB)\n",
      "\u001b[K     |████████████████████████████████| 542 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.98.0.tar.gz (535 kB)\n",
      "\u001b[K     |████████████████████████████████| 535 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.97.0.tar.gz (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.96.0.tar.gz (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.95.0.tar.gz (530 kB)\n",
      "\u001b[K     |████████████████████████████████| 530 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.94.0.tar.gz (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.93.1.tar.gz (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.93.0.tar.gz (537 kB)\n",
      "\u001b[K     |████████████████████████████████| 537 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.92.2.tar.gz (537 kB)\n",
      "\u001b[K     |████████████████████████████████| 537 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.92.1.tar.gz (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.92.0.tar.gz (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.91.1.tar.gz (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.91.0.tar.gz (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.90.0.tar.gz (534 kB)\n",
      "\u001b[K     |████████████████████████████████| 534 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.89.0.tar.gz (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.88.3.tar.gz (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.88.2.tar.gz (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.88.1.tar.gz (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.88.0.tar.gz (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.87.0.tar.gz (522 kB)\n",
      "\u001b[K     |████████████████████████████████| 522 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.86.2.tar.gz (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.86.1.tar.gz (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.86.0.tar.gz (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.85.0.tar.gz (521 kB)\n",
      "\u001b[K     |████████████████████████████████| 521 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.84.0.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.83.0.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.82.2.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.82.1.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.82.0.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.81.1.tar.gz (519 kB)\n",
      "\u001b[K     |████████████████████████████████| 519 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.81.0.tar.gz (519 kB)\n",
      "\u001b[K     |████████████████████████████████| 519 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.80.0.tar.gz (517 kB)\n",
      "\u001b[K     |████████████████████████████████| 517 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.79.0.tar.gz (516 kB)\n",
      "\u001b[K     |████████████████████████████████| 516 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.78.0.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.77.1.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.77.0.tar.gz (513 kB)\n",
      "\u001b[K     |████████████████████████████████| 513 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.76.0.tar.gz (512 kB)\n",
      "\u001b[K     |████████████████████████████████| 512 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.75.1.tar.gz (511 kB)\n",
      "\u001b[K     |████████████████████████████████| 511 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.75.0.tar.gz (511 kB)\n",
      "\u001b[K     |████████████████████████████████| 511 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.74.0.tar.gz (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.73.0.tar.gz (481 kB)\n",
      "\u001b[K     |████████████████████████████████| 481 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.3.tar.gz (475 kB)\n",
      "\u001b[K     |████████████████████████████████| 475 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.2.tar.gz (473 kB)\n",
      "\u001b[K     |████████████████████████████████| 473 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.1.tar.gz (473 kB)\n",
      "\u001b[K     |████████████████████████████████| 473 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.72.0.tar.gz (477 kB)\n",
      "\u001b[K     |████████████████████████████████| 477 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.71.0.tar.gz (477 kB)\n",
      "\u001b[K     |████████████████████████████████| 477 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.70.0.tar.gz (466 kB)\n",
      "\u001b[K     |████████████████████████████████| 466 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading sagemaker-2.69.0.tar.gz (452 kB)\n",
      "\u001b[K     |████████████████████████████████| 452 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf>=3.1\n",
      "  Downloading protobuf-4.21.6-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "\u001b[K     |████████████████████████████████| 408 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata>=1.4.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.2.9-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.48.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from packaging->transformers==4.6.1) (3.0.9)\n",
      "Requirement already satisfied: six in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers==4.6.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers==4.6.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from requests->transformers==4.6.1) (2022.6.15)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from pandas->datasets[s3]==1.6.2) (2022.1)\n",
      "Collecting ppft>=1.7.6.5\n",
      "  Downloading ppft-1.7.6.5-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pox>=0.3.1\n",
      "  Downloading pox-0.3.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.8.1-py3-none-any.whl (27 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.8.1-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.8.0-py3-none-any.whl (27 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.8.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\n",
      "Collecting aiobotocore~=2.3.4\n",
      "  Downloading aiobotocore-2.3.4-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=2.3.0\n",
      "  Downloading aiobotocore-2.3.3.tar.gz (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.2.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.1.tar.gz (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-2.3.0.tar.gz (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.3.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=2.2.0\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 5.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.2.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.0.1\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 635 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 343 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 2.8 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 2.2 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[K     |████████████████████████████████| 262 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/dnautiyal/anaconda3/envs/mlops-course/lib/python3.8/site-packages (from aioitertools>=0.5.1->aiobotocore~=2.4.0->s3fs->datasets[s3]==1.6.2) (4.3.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker, protobuf3-to-dict, aiobotocore, sacremoses\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.69.0-py2.py3-none-any.whl size=625708 sha256=c4beb81f9a65fcb8ce63ce36c6033feeaa52c0c806a2658fb02a8b161d39b73a\n",
      "  Stored in directory: /home/dnautiyal/.cache/pip/wheels/f9/f7/0d/6c7262cf3fc72a5d191e77b810901e1d9f84b9399dc00a74c4\n",
      "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4030 sha256=fdfc46e30a2b1a0dd2eeaab2106195ed78e5622ccfa23f92d4cee7c0a1dc3d64\n",
      "  Stored in directory: /home/dnautiyal/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45750 sha256=c2455926bc2afb365a86b238247a04b3f2d60c334d49daf3a8aa1d7a807c0c2c\n",
      "  Stored in directory: /home/dnautiyal/.cache/pip/wheels/18/fc/15/28be60cc5d9b8a33da2fbeeb812e7906104d00f307d8c30953\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7e13964890bca9423ee2c1e3338dbfe8e6e527ab4451e854a4661df364bccd7b\n",
      "  Stored in directory: /home/dnautiyal/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
      "Successfully built sagemaker protobuf3-to-dict aiobotocore sacremoses\n",
      "Installing collected packages: multidict, frozenlist, yarl, jmespath, async-timeout, aiosignal, wrapt, tqdm, dill, botocore, aioitertools, aiohttp, xxhash, s3transfer, pyarrow, protobuf, ppft, pox, multiprocess, joblib, huggingface-hub, fsspec, click, aiobotocore, tokenizers, smdebug-rulesconfig, sacremoses, s3fs, protobuf3-to-dict, pathos, importlib-metadata, google-pasta, datasets, boto3, transformers, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.9.1\n",
      "    Uninstalling huggingface-hub-0.9.1:\n",
      "      Successfully uninstalled huggingface-hub-0.9.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.22.0\n",
      "    Uninstalling transformers-4.22.0:\n",
      "      Successfully uninstalled transformers-4.22.0\n",
      "Successfully installed aiobotocore-1.2.2 aiohttp-3.8.1 aioitertools-0.10.0 aiosignal-1.2.0 async-timeout-4.0.2 boto3-1.16.43 botocore-1.19.52 click-8.1.3 datasets-1.6.2 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2021.7.0 google-pasta-0.2.0 huggingface-hub-0.0.8 importlib-metadata-4.12.0 jmespath-0.10.0 joblib-1.1.0 multidict-6.0.2 multiprocess-0.70.13 pathos-0.2.9 pox-0.3.1 ppft-1.7.6.5 protobuf-4.21.6 protobuf3-to-dict-0.1.5 pyarrow-9.0.0 s3fs-2021.7.0 s3transfer-0.3.7 sacremoses-0.0.53 sagemaker-2.69.0 smdebug-rulesconfig-1.0.1 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 wrapt-1.14.1 xxhash-3.0.0 yarl-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: AmazonSageMaker-ExecutionRole-20220420T205328\n",
      "sagemaker bucket: deepak-mlops4-dev\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# to install AWS CLI https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "# to configure AWS CLI https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = 'deepak-mlops4-dev'\n",
    "\n",
    "role = 'AmazonSageMaker-ExecutionRole-20220420T205328'\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `emotion` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [emotion](https://github.com/dair-ai/emotion_dataset) dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'emotion'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b2934d1d1a4fa989bd1dd8681a221e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e368233119e42af889a8ae597203f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb802a22ae945228b26bd1ff87565e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292ccd00ec424497b16eb971ff5eaadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa0fefb2614dedadffc351760706e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1600.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff2a9058aaf4a8eadca9e23d3438c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1649.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /home/dnautiyal/.cache/huggingface/datasets/emotion/default/0.0.0/6e4212efe64fd33728549b8f0435c73081391d543b596a05936857df98acb681...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ff0d32b9bf4498b9f71f13220cde6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1658616.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5107e1c2a18445f28f689aeb1713fe9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=204240.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1295e515e9c04944a4285d24e21f834d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=206760.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606f56dcf33241fc951fe3cc0a23edef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c463d5f7b40040b68da226da7e8e4f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4639ba8a4d4d96b130e75a9bc731f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f81b7057550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /home/dnautiyal/.cache/huggingface/datasets/emotion/default/0.0.0/6e4212efe64fd33728549b8f0435c73081391d543b596a05936857df98acb681. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0e403f13a24b869fb895eca0dc5a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a822c54772948c1893964b4117fa576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://deepak-mlops4-dev/samples/datasets/emotion/train\n",
      "s3://deepak-mlops4-dev/samples/datasets/emotion/test\n"
     ]
    }
   ],
   "source": [
    "print(training_input_path)\n",
    "print(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(eval_pred):\n",
      "        predictions, labels = eval_pred\n",
      "        predictions = np.argmax(predictions, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m metric.compute(predictions=predictions, references=labels)\n",
      "\n",
      "    \u001b[37m# Prepare model labels - useful in inference API\u001b[39;49;00m\n",
      "    labels = train_dataset.features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].names\n",
      "    num_labels = \u001b[36mlen\u001b[39;49;00m(labels)\n",
      "    label2id, id2label = \u001b[36mdict\u001b[39;49;00m(), \u001b[36mdict\u001b[39;49;00m()\n",
      "    \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels):\n",
      "        label2id[label] = \u001b[36mstr\u001b[39;49;00m(i)\n",
      "        id2label[\u001b[36mstr\u001b[39;49;00m(i)] = label\n",
      "\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\n",
      "        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
      "    )\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.output_dir,\n",
      "        overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m get_last_checkpoint(args.output_dir) \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        fp16=args.fp16,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        save_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        save_total_limit=\u001b[34m2\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        load_best_model_at_end=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        metric_for_best_model=\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m get_last_checkpoint(args.output_dir) \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** continue training *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        last_checkpoint = get_last_checkpoint(args.output_dir)\n",
      "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3 uses os.environ[\"SM_MODEL_DIR\"] to make sure checkpointing works\u001b[39;49;00m\n",
      "    trainer.save_model(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of supported models: https://huggingface.co/models?library=pytorch,transformers&sort=downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 32,               # batch size for training\n",
    "                 'eval_batch_size': 64,                # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                # learning rate used during training\n",
    "                 'model_id':'distilbert-base-uncased', # pre-trained model\n",
    "                 'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-workshop-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6.1',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7.1',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-16 01:32:50 Starting - Starting the training job...\n",
      "2022-09-16 01:33:17 Starting - Preparing the instances for trainingProfilerReport-1663291969: InProgress\n",
      ".........\n",
      "2022-09-16 01:34:39 Downloading - Downloading input data\n",
      "2022-09-16 01:34:39 Training - Downloading the training image.................bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-09-16 01:37:37,870 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-09-16 01:37:37,893 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-09-16 01:37:37,900 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-09-16 01:37:38,312 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 64,\n",
      "        \"fp16\": true,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_id\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-workshop-2022-09-15-21-32-4-2022-09-16-01-32-49-441\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-681261969843/huggingface-workshop-2022-09-15-21-32-4-2022-09-16-01-32-49-441/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"train_batch_size\":32}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=16\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-681261969843/huggingface-workshop-2022-09-15-21-32-4-2022-09-16-01-32-49-441/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-workshop-2022-09-15-21-32-4-2022-09-16-01-32-49-441\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-681261969843/huggingface-workshop-2022-09-15-21-32-4-2022-09-16-01-32-49-441/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"64\",\"--fp16\",\"True\",\"--learning_rate\",\"3e-05\",\"--model_id\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_EVAL_BATCH_SIZE=64\n",
      "SM_HP_FP16=true\n",
      "SM_HP_LEARNING_RATE=3e-05\n",
      "SM_HP_MODEL_ID=distilbert-base-uncased\n",
      "SM_HP_TRAIN_BATCH_SIZE=32\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.6 train.py --epochs 1 --eval_batch_size 64 --fp16 True --learning_rate 3e-05 --model_id distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "2022-09-16 01:37:58 Training - Training image download completed. Training in progress.2022-09-16 01:37:42,331 - __main__ - INFO -  loaded train_dataset length is: 16000\n",
      "2022-09-16 01:37:42,331 - __main__ - INFO -  loaded test_dataset length is: 2000\n",
      "2022-09-16 01:37:42,529 - filelock - INFO - Lock 140348540234216 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\n",
      "2022-09-16 01:37:42,551 - filelock - INFO - Lock 140348540234216 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\n",
      "2022-09-16 01:37:42,573 - filelock - INFO - Lock 140348529880872 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "2022-09-16 01:37:46,446 - filelock - INFO - Lock 140348529880872 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-09-16 01:37:46,922 - filelock - INFO - Lock 140348526042752 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "2022-09-16 01:37:46,955 - filelock - INFO - Lock 140348526042752 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "2022-09-16 01:37:46,973 - filelock - INFO - Lock 140351983770536 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "2022-09-16 01:37:47,017 - filelock - INFO - Lock 140351983770536 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "2022-09-16 01:37:47,074 - filelock - INFO - Lock 140348526042864 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "2022-09-16 01:37:47,094 - filelock - INFO - Lock 140348526042864 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "[2022-09-16 01:37:50.421 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-16 01:37:50.545 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\n",
      "[2022-09-16 01:37:50.546 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2022-09-16 01:37:50.546 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2022-09-16 01:37:50.547 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\n",
      "[2022-09-16 01:37:50.547 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2022-09-16 01:37:50.660 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.661 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.662 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.663 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.664 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.665 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.666 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.667 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.668 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.669 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\n",
      "[2022-09-16 01:37:50.670 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:pre_classifier.weight count_params:589824\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:pre_classifier.bias count_params:768\n",
      "[2022-09-16 01:37:50.671 algo-1:26 INFO hook.py:591] name:classifier.weight count_params:4608\n",
      "[2022-09-16 01:37:50.672 algo-1:26 INFO hook.py:591] name:classifier.bias count_params:6\n",
      "[2022-09-16 01:37:50.672 algo-1:26 INFO hook.py:593] Total Trainable Params: 66958086\n",
      "[2022-09-16 01:37:50.672 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\n",
      "[2022-09-16 01:37:50.673 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\n",
      "{'loss': 0.9431, 'learning_rate': 2.9880000000000002e-05, 'epoch': 1.0}\n",
      "2022-09-16 01:40:52,011 - /opt/conda/lib/python3.6/site-packages/datasets/metric.py - INFO - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "{'eval_loss': 0.25532859563827515, 'eval_accuracy': 0.9125, 'eval_runtime': 7.4291, 'eval_samples_per_second': 269.211, 'epoch': 1.0}\n",
      "{'train_runtime': 183.0727, 'train_samples_per_second': 2.731, 'epoch': 1.0}\n",
      "2022-09-16 01:41:00,911 - /opt/conda/lib/python3.6/site-packages/datasets/metric.py - INFO - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "***** Eval results *****\n",
      "epoch = 1.0\n",
      "eval_accuracy = 0.9125\n",
      "eval_loss = 0.25532859563827515\n",
      "eval_mem_cpu_alloc_delta = 0\n",
      "eval_mem_cpu_peaked_delta = 0\n",
      "eval_mem_gpu_alloc_delta = 0\n",
      "eval_mem_gpu_peaked_delta = 2315885568\n",
      "eval_runtime = 7.4237\n",
      "eval_samples_per_second = 269.406\n",
      "2022-09-16 01:41:02,152 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "#015Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]#015Downloading: 2.92kB [00:00, 2.05MB/s]                   \n",
      "#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 825kB/s]\n",
      "#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   3%|▎         | 6.88M/268M [00:00<00:03, 68.8MB/s]#015Downloading:   5%|▌         | 13.8M/268M [00:00<00:03, 68.8MB/s]#015Downloading:   8%|▊         | 20.7M/268M [00:00<00:03, 69.0MB/s]#015Downloading:  10%|█         | 27.6M/268M [00:00<00:03, 69.0MB/s]#015Downloading:  13%|█▎        | 34.6M/268M [00:00<00:03, 69.3MB/s]#015Downloading:  16%|█▌        | 41.6M/268M [00:00<00:03, 69.5MB/s]#015Downloading:  18%|█▊        | 48.6M/268M [00:00<00:03, 69.7MB/s]#015Downloading:  21%|██        | 55.6M/268M [00:00<00:03, 69.7MB/s]#015Downloading:  23%|██▎       | 62.6M/268M [00:00<00:02, 69.8MB/s]#015Downloading:  26%|██▌       | 69.6M/268M [00:01<00:02, 69.9MB/s]#015Downloading:  29%|██▊       | 76.6M/268M [00:01<00:02, 69.8MB/s]#015Downloading:  31%|███       | 83.6M/268M [00:01<00:02, 69.9MB/s]#015Downloading:  34%|███▍      | 90.6M/268M [00:01<00:02, 69.9MB/s]#015Downloading:  36%|███▋      | 97.6M/268M [00:01<00:02, 70.0MB/s]#015Downloading:  39%|███▉      | 105M/268M [00:01<00:02, 70.0MB/s] #015Downloading:  42%|████▏     | 112M/268M [00:01<00:02, 70.1MB/s]#015Downloading:  44%|████▍     | 119M/268M [00:01<00:02, 70.0MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:01<00:02, 68.6MB/s]#015Downloading:  49%|████▉     | 133M/268M [00:01<00:01, 68.9MB/s]#015Downloading:  52%|█████▏    | 140M/268M [00:02<00:01, 69.2MB/s]#015Downloading:  55%|█████▍    | 147M/268M [00:02<00:01, 69.4MB/s]#015Downloading:  57%|█████▋    | 154M/268M [00:02<00:01, 69.5MB/s]#015Downloading:  60%|█████▉    | 160M/268M [00:02<00:01, 69.5MB/s]#015Downloading:  62%|██████▏   | 167M/268M [00:02<00:01, 69.4MB/s]#015Downloading:  65%|██████▌   | 174M/268M [00:02<00:01, 69.3MB/s]#015Downloading:  68%|██████▊   | 181M/268M [00:02<00:01, 69.4MB/s]#015Downloading:  70%|███████   | 188M/268M [00:02<00:01, 69.0MB/s]#015Downloading:  73%|███████▎  | 195M/268M [00:02<00:01, 68.9MB/s]#015Downloading:  75%|███████▌  | 202M/268M [00:02<00:00, 68.8MB/s]#015Downloading:  78%|███████▊  | 209M/268M [00:03<00:00, 69.2MB/s]#015Downloading:  81%|████████  | 216M/268M [00:03<00:00, 69.3MB/s]#015Downloading:  83%|████████▎ | 223M/268M [00:03<00:00, 69.4MB/s]#015Downloading:  86%|████████▌ | 230M/268M [00:03<00:00, 69.4MB/s]#015Downloading:  88%|████████▊ | 237M/268M [00:03<00:00, 69.6MB/s]#015Downloading:  91%|█████████ | 244M/268M [00:03<00:00, 69.7MB/s]#015Downloading:  94%|█████████▎| 251M/268M [00:03<00:00, 69.8MB/s]#015Downloading:  96%|█████████▌| 258M/268M [00:03<00:00, 69.9MB/s]#015Downloading:  99%|█████████▉| 265M/268M [00:03<00:00, 69.9MB/s]#015Downloading: 100%|██████████| 268M/268M [00:03<00:00, 69.5MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 55.6MB/s]\n",
      "#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 63.8MB/s]\n",
      "#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 41.1kB/s]\n",
      "#015  0%|          | 0/500 [00:00<?, ?it/s]#015  0%|          | 1/500 [00:01<16:03,  1.93s/it]#015  0%|          | 2/500 [00:02<12:04,  1.45s/it]#015  1%|          | 3/500 [00:02<09:16,  1.12s/it]#015  1%|          | 4/500 [00:02<07:19,  1.13it/s]#015  1%|          | 5/500 [00:03<05:57,  1.39it/s]#015  1%|          | 6/500 [00:03<04:59,  1.65it/s]#015  1%|▏         | 7/500 [00:03<04:18,  1.91it/s]#015  2%|▏         | 8/500 [00:04<03:58,  2.06it/s]#015  2%|▏         | 9/500 [00:04<03:37,  2.26it/s]#015  2%|▏         | 10/500 [00:05<03:21,  2.43it/s]#015  2%|▏         | 11/500 [00:05<03:11,  2.55it/s]#015  2%|▏         | 12/500 [00:05<03:06,  2.62it/s]#015  3%|▎         | 13/500 [00:06<03:00,  2.70it/s]#015  3%|▎         | 14/500 [00:06<02:56,  2.75it/s]#015  3%|▎         | 15/500 [00:06<02:55,  2.76it/s]#015  3%|▎         | 16/500 [00:07<02:54,  2.78it/s]#015  3%|▎         | 17/500 [00:07<02:50,  2.83it/s]#015  4%|▎         | 18/500 [00:07<02:48,  2.86it/s]#015  4%|▍         | 19/500 [00:08<02:46,  2.89it/s]#015  4%|▍         | 20/500 [00:08<02:46,  2.88it/s]#015  4%|▍         | 21/500 [00:08<02:44,  2.91it/s]#015  4%|▍         | 22/500 [00:09<02:43,  2.93it/s]#015  5%|▍         | 23/500 [00:09<02:42,  2.93it/s]#015  5%|▍         | 24/500 [00:09<02:43,  2.92it/s]#015  5%|▌         | 25/500 [00:10<02:43,  2.91it/s]#015  5%|▌         | 26/500 [00:10<02:44,  2.88it/s]#015  5%|▌         | 27/500 [00:10<02:43,  2.89it/s]#015  6%|▌         | 28/500 [00:11<02:44,  2.88it/s]#015  6%|▌         | 29/500 [00:11<02:46,  2.82it/s]#015  6%|▌         | 30/500 [00:11<02:47,  2.81it/s]#015  6%|▌         | 31/500 [00:12<02:46,  2.81it/s]#015  6%|▋         | 32/500 [00:12<02:46,  2.80it/s]#015  7%|▋         | 33/500 [00:13<02:45,  2.83it/s]#015  7%|▋         | 34/500 [00:13<02:42,  2.87it/s]#015  7%|▋         | 35/500 [00:13<02:40,  2.90it/s]#015  7%|▋         | 36/500 [00:14<02:42,  2.86it/s]#015  7%|▋         | 37/500 [00:14<02:43,  2.84it/s]#015  8%|▊         | 38/500 [00:14<02:43,  2.82it/s]#015  8%|▊         | 39/500 [00:15<02:47,  2.76it/s]#015  8%|▊         | 40/500 [00:15<02:46,  2.77it/s]#015  8%|▊         | 41/500 [00:15<02:42,  2.83it/s]#015  8%|▊         | 42/500 [00:16<02:40,  2.86it/s]#015  9%|▊         | 43/500 [00:16<02:38,  2.88it/s]#015  9%|▉         | 44/500 [00:16<02:38,  2.89it/s]#015  9%|▉         | 45/500 [00:17<02:37,  2.90it/s]#015  9%|▉         | 46/500 [00:17<02:37,  2.89it/s]#015  9%|▉         | 47/500 [00:17<02:36,  2.90it/s]#015 10%|▉         | 48/500 [00:18<02:35,  2.91it/s]#015 10%|▉         | 49/500 [00:18<02:35,  2.91it/s]#015 10%|█         | 50/500 [00:18<02:33,  2.93it/s]#015 10%|█         | 51/500 [00:19<02:33,  2.93it/s]#015 10%|█         | 52/500 [00:19<02:33,  2.91it/s]#015 11%|█         | 53/500 [00:19<02:33,  2.91it/s]#015 11%|█         | 54/500 [00:20<02:33,  2.91it/s]#015 11%|█         | 55/500 [00:20<02:33,  2.89it/s]#015 11%|█         | 56/500 [00:21<02:32,  2.91it/s]#015 11%|█▏        | 57/500 [00:21<02:31,  2.93it/s]#015 12%|█▏        | 58/500 [00:21<02:34,  2.86it/s]#015 12%|█▏        | 59/500 [00:22<02:33,  2.87it/s]#015 12%|█▏        | 60/500 [00:22<02:32,  2.89it/s]#015 12%|█▏        | 61/500 [00:22<02:31,  2.90it/s]#015 12%|█▏        | 62/500 [00:23<02:30,  2.92it/s]#015 13%|█▎        | 63/500 [00:23<02:30,  2.91it/s]#015 13%|█▎        | 64/500 [00:23<02:30,  2.90it/s]#015 13%|█▎        | 65/500 [00:24<02:29,  2.90it/s]#015 13%|█▎        | 66/500 [00:24<02:29,  2.90it/s]#015 13%|█▎        | 67/500 [00:24<02:28,  2.92it/s]#015 14%|█▎        | 68/500 [00:25<02:27,  2.93it/s]#015 14%|█▍        | 69/500 [00:25<02:26,  2.94it/s]#015 14%|█▍        | 70/500 [00:25<02:26,  2.93it/s]#015 14%|█▍        | 71/500 [00:26<02:26,  2.93it/s]#015 14%|█▍        | 72/500 [00:26<02:26,  2.93it/s]#015 15%|█▍        | 73/500 [00:26<02:25,  2.94it/s]#015 15%|█▍        | 74/500 [00:27<02:25,  2.93it/s]#015 15%|█▌        | 75/500 [00:27<02:25,  2.92it/s]#015 15%|█▌        | 76/500 [00:27<02:24,  2.92it/s]#015 15%|█▌        | 77/500 [00:28<02:24,  2.93it/s]#015 16%|█▌        | 78/500 [00:28<02:23,  2.94it/s]#015 16%|█▌        | 79/500 [00:28<02:23,  2.94it/s]#015 16%|█▌        | 80/500 [00:29<02:22,  2.95it/s]#015 16%|█▌        | 81/500 [00:29<02:22,  2.95it/s]#015 16%|█▋        | 82/500 [00:29<02:21,  2.96it/s]#015 17%|█▋        | 83/500 [00:30<02:22,  2.93it/s]#015 17%|█▋        | 84/500 [00:30<02:21,  2.93it/s]#015 17%|█▋        | 85/500 [00:30<02:21,  2.92it/s]#015 17%|█▋        | 86/500 [00:31<02:20,  2.94it/s]#015 17%|█▋        | 87/500 [00:31<02:20,  2.93it/s]#015 18%|█▊        | 88/500 [00:31<02:20,  2.94it/s]#015 18%|█▊        | 89/500 [00:32<02:19,  2.94it/s]#015 18%|█▊        | 90/500 [00:32<02:19,  2.93it/s]#015 18%|█▊        | 91/500 [00:33<02:20,  2.91it/s]#015 18%|█▊        | 92/500 [00:33<02:19,  2.92it/s]#015 19%|█▊        | 93/500 [00:33<02:18,  2.93it/s]#015 19%|█▉        | 94/500 [00:34<02:20,  2.90it/s]#015 19%|█▉        | 95/500 [00:34<02:20,  2.89it/s]#015 19%|█▉        | 96/500 [00:34<02:18,  2.91it/s]#015 19%|█▉        | 97/500 [00:35<02:23,  2.81it/s]#015 20%|█▉        | 98/500 [00:35<02:21,  2.84it/s]#015 20%|█▉        | 99/500 [00:35<02:21,  2.83it/s]#015 20%|██        | 100/500 [00:36<02:20,  2.85it/s]#015 20%|██        | 101/500 [00:36<02:19,  2.85it/s]#015 20%|██        | 102/500 [00:36<02:19,  2.85it/s]#015 21%|██        | 103/500 [00:37<02:17,  2.88it/s]#015 21%|██        | 104/500 [00:37<02:15,  2.92it/s]#015 21%|██        | 105/500 [00:37<02:15,  2.91it/s]#015 21%|██        | 106/500 [00:38<02:15,  2.91it/s]#015 21%|██▏       | 107/500 [00:38<02:14,  2.92it/s]#015 22%|██▏       | 108/500 [00:38<02:13,  2.93it/s]#015 22%|██▏       | 109/500 [00:39<02:13,  2.93it/s]#015 22%|██▏       | 110/500 [00:39<02:12,  2.94it/s]#015 22%|██▏       | 111/500 [00:39<02:12,  2.94it/s]#015 22%|██▏       | 112/500 [00:40<02:12,  2.93it/s]#015 23%|██▎       | 113/500 [00:40<02:11,  2.94it/s]#015 23%|██▎       | 114/500 [00:40<02:12,  2.91it/s]#015 23%|██▎       | 115/500 [00:41<02:13,  2.89it/s]#015 23%|██▎       | 116/500 [00:41<02:11,  2.93it/s]#015 23%|██▎       | 117/500 [00:41<02:11,  2.91it/s]#015 24%|██▎       | 118/500 [00:42<02:11,  2.91it/s]#015 24%|██▍       | 119/500 [00:42<02:11,  2.91it/s]#015 24%|██▍       | 120/500 [00:43<02:11,  2.89it/s]#015 24%|██▍       | 121/500 [00:43<02:10,  2.90it/s]#015 24%|██▍       | 122/500 [00:43<02:09,  2.93it/s]#015 25%|██▍       | 123/500 [00:44<02:08,  2.93it/s]#015 25%|██▍       | 124/500 [00:44<02:09,  2.90it/s]#015 25%|██▌       | 125/500 [00:44<02:10,  2.88it/s]#015 25%|██▌       | 126/500 [00:45<02:11,  2.85it/s]#015 25%|██▌       | 127/500 [00:45<02:13,  2.80it/s]#015 26%|██▌       | 128/500 [00:45<02:11,  2.82it/s]#015 26%|██▌       | 129/500 [00:46<02:10,  2.84it/s]#015 26%|██▌       | 130/500 [00:46<02:09,  2.86it/s]#015 26%|██▌       | 131/500 [00:46<02:08,  2.88it/s]#015 26%|██▋       | 132/500 [00:47<02:06,  2.91it/s]#015 27%|██▋       | 133/500 [00:47<02:06,  2.91it/s]#015 27%|██▋       | 134/500 [00:47<02:04,  2.93it/s]#015 27%|██▋       | 135/500 [00:48<02:07,  2.86it/s]#015 27%|██▋       | 136/500 [00:48<02:06,  2.87it/s]#015 27%|██▋       | 137/500 [00:48<02:06,  2.87it/s]#015 28%|██▊       | 138/500 [00:49<02:05,  2.89it/s]#015 28%|██▊       | 139/500 [00:49<02:04,  2.90it/s]#015 28%|██▊       | 140/500 [00:49<02:04,  2.88it/s]#015 28%|██▊       | 141/500 [00:50<02:03,  2.90it/s]#015 28%|██▊       | 142/500 [00:50<02:02,  2.91it/s]#015 29%|██▊       | 143/500 [00:50<02:02,  2.90it/s]#015 29%|██▉       | 144/500 [00:51<02:02,  2.90it/s]#015 29%|██▉       | 145/500 [00:51<02:01,  2.91it/s]#015 29%|██▉       | 146/500 [00:52<02:04,  2.84it/s]#015 29%|██▉       | 147/500 [00:52<02:03,  2.86it/s]#015 30%|██▉       | 148/500 [00:52<02:02,  2.88it/s]#015 30%|██▉       | 149/500 [00:53<02:02,  2.87it/s]#015 30%|███       | 150/500 [00:53<02:00,  2.89it/s]#015 30%|███       | 151/500 [00:53<01:59,  2.91it/s]#015 30%|███       | 152/500 [00:54<01:59,  2.92it/s]#015 31%|███       | 153/500 [00:54<02:01,  2.87it/s]#015 31%|███       | 154/500 [00:54<02:00,  2.87it/s]#015 31%|███       | 155/500 [00:55<01:59,  2.89it/s]#015 31%|███       | 156/500 [00:55<01:58,  2.90it/s]#015 31%|███▏      | 157/500 [00:55<01:57,  2.92it/s]#015 32%|███▏      | 158/500 [00:56<01:56,  2.93it/s]#015 32%|███▏      | 159/500 [00:56<01:55,  2.94it/s]#015 32%|███▏      | 160/500 [00:56<01:55,  2.95it/s]#015 32%|███▏      | 161/500 [00:57<01:54,  2.96it/s]#015 32%|███▏      | 162/500 [00:57<01:53,  2.97it/s]#015 33%|███▎      | 163/500 [00:57<01:53,  2.96it/s]#015 33%|███▎      | 164/500 [00:58<01:51,  3.02it/s]#015 33%|███▎      | 165/500 [00:58<01:51,  2.99it/s]#015 33%|███▎      | 166/500 [00:58<01:53,  2.95it/s]#015 33%|███▎      | 167/500 [00:59<01:53,  2.92it/s]#015 34%|███▎      | 168/500 [00:59<01:53,  2.93it/s]#015 34%|███▍      | 169/500 [00:59<01:52,  2.94it/s]#015 34%|███▍      | 170/500 [01:00<01:51,  2.95it/s]#015 34%|███▍      | 171/500 [01:00<01:51,  2.96it/s]#015 34%|███▍      | 172/500 [01:00<01:51,  2.95it/s]#015 35%|███▍      | 173/500 [01:01<01:50,  2.96it/s]#015 35%|███▍      | 174/500 [01:01<01:50,  2.95it/s]#015 35%|███▌      | 175/500 [01:01<01:51,  2.93it/s]#015 35%|███▌      | 176/500 [01:02<01:51,  2.91it/s]#015 35%|███▌      | 177/500 [01:02<01:50,  2.92it/s]#015 36%|███▌      | 178/500 [01:02<01:50,  2.92it/s]#015 36%|███▌      | 179/500 [01:03<01:50,  2.92it/s]#015 36%|███▌      | 180/500 [01:03<01:50,  2.90it/s]#015 36%|███▌      | 181/500 [01:03<01:50,  2.90it/s]#015 36%|███▋      | 182/500 [01:04<01:49,  2.92it/s]#015 37%|███▋      | 183/500 [01:04<01:49,  2.90it/s]#015 37%|███▋      | 184/500 [01:05<01:48,  2.92it/s]#015 37%|███▋      | 185/500 [01:05<01:47,  2.93it/s]#015 37%|███▋      | 186/500 [01:05<01:46,  2.94it/s]#015 37%|███▋      | 187/500 [01:06<01:46,  2.95it/s]#015 38%|███▊      | 188/500 [01:06<01:46,  2.94it/s]#015 38%|███▊      | 189/500 [01:06<01:46,  2.92it/s]#015 38%|███▊      | 190/500 [01:07<01:46,  2.90it/s]#015 38%|███▊      | 191/500 [01:07<01:45,  2.92it/s]#015 38%|███▊      | 192/500 [01:07<01:45,  2.93it/s]#015 39%|███▊      | 193/500 [01:08<01:44,  2.94it/s]#015 39%|███▉      | 194/500 [01:08<01:44,  2.92it/s]#015 39%|███▉      | 195/500 [01:08<01:44,  2.92it/s]#015 39%|███▉      | 196/500 [01:09<01:44,  2.91it/s]#015 39%|███▉      | 197/500 [01:09<01:44,  2.91it/s]#015 40%|███▉      | 198/500 [01:09<01:44,  2.89it/s]#015 40%|███▉      | 199/500 [01:10<01:44,  2.87it/s]#015 40%|████      | 200/500 [01:10<01:43,  2.90it/s]#015 40%|████      | 201/500 [01:10<01:43,  2.88it/s]#015 40%|████      | 202/500 [01:11<01:43,  2.88it/s]#015 41%|████      | 203/500 [01:11<01:42,  2.90it/s]#015 41%|████      | 204/500 [01:11<01:41,  2.92it/s]#015 41%|████      | 205/500 [01:12<01:42,  2.89it/s]#015 41%|████      | 206/500 [01:12<01:41,  2.90it/s]#015 41%|████▏     | 207/500 [01:12<01:40,  2.91it/s]#015 42%|████▏     | 208/500 [01:13<01:40,  2.91it/s]#015 42%|████▏     | 209/500 [01:13<01:39,  2.92it/s]#015 42%|████▏     | 210/500 [01:13<01:40,  2.90it/s]#015 42%|████▏     | 211/500 [01:14<01:40,  2.87it/s]#015 42%|████▏     | 212/500 [01:14<01:39,  2.90it/s]#015 43%|████▎     | 213/500 [01:14<01:39,  2.87it/s]#015 43%|████▎     | 214/500 [01:15<01:40,  2.85it/s]#015 43%|████▎     | 215/500 [01:15<01:39,  2.87it/s]#015 43%|████▎     | 216/500 [01:16<01:38,  2.89it/s]#015 43%|████▎     | 217/500 [01:16<01:38,  2.88it/s]#015 44%|████▎     | 218/500 [01:16<01:38,  2.86it/s]#015 44%|████▍     | 219/500 [01:17<01:38,  2.86it/s]#015 44%|████▍     | 220/500 [01:17<01:36,  2.89it/s]#015 44%|████▍     | 221/500 [01:17<01:37,  2.87it/s]#015 44%|████▍     | 222/500 [01:18<01:37,  2.86it/s]#015 45%|████▍     | 223/500 [01:18<01:36,  2.86it/s]#015 45%|████▍     | 224/500 [01:18<01:36,  2.85it/s]#015 45%|████▌     | 225/500 [01:19<01:36,  2.86it/s]#015 45%|████▌     | 226/500 [01:19<01:34,  2.89it/s]#015 45%|████▌     | 227/500 [01:19<01:33,  2.91it/s]#015 46%|████▌     | 228/500 [01:20<01:33,  2.92it/s]#015 46%|████▌     | 229/500 [01:20<01:33,  2.91it/s]#015 46%|████▌     | 230/500 [01:20<01:32,  2.91it/s]#015 46%|████▌     | 231/500 [01:21<01:32,  2.91it/s]#015 46%|████▋     | 232/500 [01:21<01:31,  2.92it/s]#015 47%|████▋     | 233/500 [01:21<01:30,  2.94it/s]#015 47%|████▋     | 234/500 [01:22<01:30,  2.93it/s]#015 47%|████▋     | 235/500 [01:22<01:30,  2.94it/s]#015 47%|████▋     | 236/500 [01:22<01:29,  2.95it/s]#015 47%|████▋     | 237/500 [01:23<01:29,  2.94it/s]#015 48%|████▊     | 238/500 [01:23<01:29,  2.94it/s]#015 48%|████▊     | 239/500 [01:23<01:30,  2.90it/s]#015 48%|████▊     | 240/500 [01:24<01:30,  2.86it/s]#015 48%|████▊     | 241/500 [01:24<01:30,  2.85it/s]#015 48%|████▊     | 242/500 [01:25<01:30,  2.86it/s]#015 49%|████▊     | 243/500 [01:25<01:28,  2.90it/s]#015 49%|████▉     | 244/500 [01:25<01:27,  2.93it/s]#015 49%|████▉     | 245/500 [01:26<01:26,  2.95it/s]#015 49%|████▉     | 246/500 [01:26<01:27,  2.91it/s]#015 49%|████▉     | 247/500 [01:26<01:26,  2.92it/s]#015 50%|████▉     | 248/500 [01:27<01:26,  2.92it/s]#015 50%|████▉     | 249/500 [01:27<01:26,  2.91it/s]#015 50%|█████     | 250/500 [01:27<01:25,  2.93it/s]#015 50%|█████     | 251/500 [01:28<01:24,  2.95it/s]#015 50%|█████     | 252/500 [01:28<01:24,  2.95it/s]#015 51%|█████     | 253/500 [01:28<01:23,  2.94it/s]#015 51%|█████     | 254/500 [01:29<01:23,  2.95it/s]#015 51%|█████     | 255/500 [01:29<01:22,  2.96it/s]#015 51%|█████     | 256/500 [01:29<01:23,  2.90it/s]#015 51%|█████▏    | 257/500 [01:30<01:23,  2.92it/s]#015 52%|█████▏    | 258/500 [01:30<01:22,  2.94it/s]#015 52%|█████▏    | 259/500 [01:30<01:22,  2.94it/s]#015 52%|█████▏    | 260/500 [01:31<01:21,  2.94it/s]#015 52%|█████▏    | 261/500 [01:31<01:21,  2.94it/s]#015 52%|█████▏    | 262/500 [01:31<01:21,  2.94it/s]#015 53%|█████▎    | 263/500 [01:32<01:20,  2.93it/s]#015 53%|█████▎    | 264/500 [01:32<01:20,  2.94it/s]#015 53%|█████▎    | 265/500 [01:32<01:19,  2.95it/s]#015 53%|█████▎    | 266/500 [01:33<01:19,  2.93it/s]#015 53%|█████▎    | 267/500 [01:33<01:19,  2.94it/s]#015 54%|█████▎    | 268/500 [01:33<01:18,  2.95it/s]#015 54%|█████▍    | 269/500 [01:34<01:18,  2.95it/s]#015 54%|█████▍    | 270/500 [01:34<01:18,  2.95it/s]#015 54%|█████▍    | 271/500 [01:34<01:17,  2.95it/s]#015 54%|█████▍    | 272/500 [01:35<01:16,  2.96it/s]#015 55%|█████▍    | 273/500 [01:35<01:17,  2.94it/s]#015 55%|█████▍    | 274/500 [01:35<01:16,  2.95it/s]#015 55%|█████▌    | 275/500 [01:36<01:16,  2.95it/s]#015 55%|█████▌    | 276/500 [01:36<01:15,  2.95it/s]#015 55%|█████▌    | 277/500 [01:36<01:15,  2.96it/s]#015 56%|█████▌    | 278/500 [01:37<01:14,  2.96it/s]#015 56%|█████▌    | 279/500 [01:37<01:15,  2.94it/s]#015 56%|█████▌    | 280/500 [01:37<01:14,  2.96it/s]#015 56%|█████▌    | 281/500 [01:38<01:13,  2.97it/s]#015 56%|█████▋    | 282/500 [01:38<01:13,  2.96it/s]#015 57%|█████▋    | 283/500 [01:38<01:14,  2.92it/s]#015 57%|█████▋    | 284/500 [01:39<01:13,  2.93it/s]#015 57%|█████▋    | 285/500 [01:39<01:13,  2.94it/s]#015 57%|█████▋    | 286/500 [01:39<01:13,  2.91it/s]#015 57%|█████▋    | 287/500 [01:40<01:13,  2.90it/s]#015 58%|█████▊    | 288/500 [01:40<01:13,  2.89it/s]#015 58%|█████▊    | 289/500 [01:41<01:12,  2.91it/s]#015 58%|█████▊    | 290/500 [01:41<01:11,  2.92it/s]#015 58%|█████▊    | 291/500 [01:41\n",
      "<01:13,  2.83it/s]#015 58%|█████▊    | 292/500 [01:42<01:14,  2.80it/s]#015 59%|█████▊    | 293/500 [01:42<01:12,  2.84it/s]#015 59%|█████▉    | 294/500 [01:42<01:11,  2.87it/s]#015 59%|█████▉    | 295/500 [01:43<01:11,  2.86it/s]#015 59%|█████▉    | 296/500 [01:43<01:10,  2.89it/s]#015 59%|█████▉    | 297/500 [01:43<01:10,  2.87it/s]#015 60%|█████▉    | 298/500 [01:44<01:09,  2.89it/s]#015 60%|█████▉    | 299/500 [01:44<01:09,  2.89it/s]#015 60%|██████    | 300/500 [01:44<01:10,  2.84it/s]#015 60%|██████    | 301/500 [01:45<01:09,  2.87it/s]#015 60%|██████    | 302/500 [01:45<01:09,  2.86it/s]#015 61%|██████    | 303/500 [01:45<01:08,  2.87it/s]#015 61%|██████    | 304/500 [01:46<01:07,  2.89it/s]#015 61%|██████    | 305/500 [01:46<01:07,  2.90it/s]#015 61%|██████    | 306/500 [01:46<01:06,  2.91it/s]#015 61%|██████▏   | 307/500 [01:47<01:05,  2.93it/s]#015 62%|██████▏   | 308/500 [01:47<01:06,  2.90it/s]#015 62%|██████▏   | 309/500 [01:47<01:05,  2.91it/s]#015 62%|██████▏   | 310/500 [01:48<01:04,  2.93it/s]#015 62%|██████▏   | 311/500 [01:48<01:04,  2.94it/s]#015 62%|██████▏   | 312/500 [01:48<01:03,  2.94it/s]#015 63%|██████▎   | 313/500 [01:49<01:03,  2.93it/s]#015 63%|██████▎   | 314/500 [01:49<01:03,  2.93it/s]#015 63%|██████▎   | 315/500 [01:50<01:03,  2.93it/s]#015 63%|██████▎   | 316/500 [01:50<01:03,  2.90it/s]#015 63%|██████▎   | 317/500 [01:50<01:02,  2.91it/s]#015 64%|██████▎   | 318/500 [01:51<01:02,  2.92it/s]#015 64%|██████▍   | 319/500 [01:51<01:01,  2.93it/s]#015 64%|██████▍   | 320/500 [01:51<01:02,  2.87it/s]#015 64%|██████▍   | 321/500 [01:52<01:01,  2.90it/s]#015 64%|██████▍   | 322/500 [01:52<01:00,  2.92it/s]#015 65%|██████▍   | 323/500 [01:52<01:00,  2.93it/s]#015 65%|██████▍   | 324/500 [01:53<00:59,  2.94it/s]#015 65%|██████▌   | 325/500 [01:53<00:59,  2.95it/s]#015 65%|██████▌   | 326/500 [01:53<00:59,  2.92it/s]#015 65%|██████▌   | 327/500 [01:54<00:59,  2.89it/s]#015 66%|██████▌   | 328/500 [01:54<01:00,  2.86it/s]#015 66%|██████▌   | 329/500 [01:54<00:59,  2.88it/s]#015 66%|██████▌   | 330/500 [01:55<01:00,  2.83it/s]#015 66%|██████▌   | 331/500 [01:55<01:01,  2.74it/s]#015 66%|██████▋   | 332/500 [01:55<01:00,  2.79it/s]#015 67%|██████▋   | 333/500 [01:56<00:59,  2.81it/s]#015 67%|██████▋   | 334/500 [01:56<00:59,  2.80it/s]#015 67%|██████▋   | 335/500 [01:56<00:58,  2.84it/s]#015 67%|██████▋   | 336/500 [01:57<00:57,  2.84it/s]#015 67%|██████▋   | 337/500 [01:57<00:56,  2.87it/s]#015 68%|██████▊   | 338/500 [01:57<00:54,  2.97it/s]#015 68%|██████▊   | 339/500 [01:58<00:54,  2.95it/s]#015 68%|██████▊   | 340/500 [01:58<00:54,  2.92it/s]#015 68%|██████▊   | 341/500 [01:59<00:54,  2.93it/s]#015 68%|██████▊   | 342/500 [01:59<00:53,  2.95it/s]#015 69%|██████▊   | 343/500 [01:59<00:53,  2.93it/s]#015 69%|██████▉   | 344/500 [02:00<00:53,  2.91it/s]#015 69%|██████▉   | 345/500 [02:00<00:53,  2.89it/s]#015 69%|██████▉   | 346/500 [02:00<00:52,  2.92it/s]#015 69%|██████▉   | 347/500 [02:01<00:52,  2.91it/s]#015 70%|██████▉   | 348/500 [02:01<00:52,  2.91it/s]#015 70%|██████▉   | 349/500 [02:01<00:52,  2.89it/s]#015 70%|███████   | 350/500 [02:02<00:52,  2.86it/s]#015 70%|███████   | 351/500 [02:02<00:51,  2.89it/s]#015 70%|███████   | 352/500 [02:02<00:50,  2.90it/s]#015 71%|███████   | 353/500 [02:03<00:51,  2.88it/s]#015 71%|███████   | 354/500 [02:03<00:50,  2.89it/s]#015 71%|███████   | 355/500 [02:03<00:49,  2.90it/s]#015 71%|███████   | 356/500 [02:04<00:49,  2.92it/s]#015 71%|███████▏  | 357/500 [02:04<00:48,  2.92it/s]#015 72%|███████▏  | 358/500 [02:04<00:49,  2.89it/s]#015 72%|███████▏  | 359/500 [02:05<00:48,  2.91it/s]#015 72%|███████▏  | 360/500 [02:05<00:48,  2.90it/s]#015 72%|███████▏  | 361/500 [02:05<00:47,  2.92it/s]#015 72%|███████▏  | 362/500 [02:06<00:47,  2.93it/s]#015 73%|███████▎  | 363/500 [02:06<00:46,  2.92it/s]#015 73%|███████▎  | 364/500 [02:06<00:46,  2.91it/s]#015 73%|███████▎  | 365/500 [02:07<00:46,  2.93it/s]#015 73%|███████▎  | 366/500 [02:07<00:45,  2.94it/s]#015 73%|███████▎  | 367/500 [02:07<00:45,  2.94it/s]#015 74%|███████▎  | 368/500 [02:08<00:46,  2.84it/s]#015 74%|███████▍  | 369/500 [02:08<00:45,  2.87it/s]#015 74%|███████▍  | 370/500 [02:09<00:44,  2.89it/s]#015 74%|███████▍  | 371/500 [02:09<00:44,  2.91it/s]#015 74%|███████▍  | 372/500 [02:09<00:44,  2.90it/s]#015 75%|███████▍  | 373/500 [02:10<00:43,  2.91it/s]#015 75%|███████▍  | 374/500 [02:10<00:43,  2.92it/s]#015 75%|███████▌  | 375/500 [02:10<00:42,  2.91it/s]#015 75%|███████▌  | 376/500 [02:11<00:42,  2.91it/s]#015 75%|███████▌  | 377/500 [02:11<00:42,  2.93it/s]#015 76%|███████▌  | 378/500 [02:11<00:42,  2.84it/s]#015 76%|███████▌  | 379/500 [02:12<00:43,  2.81it/s]#015 76%|███████▌  | 380/500 [02:12<00:42,  2.83it/s]#015 76%|███████▌  | 381/500 [02:12<00:41,  2.84it/s]#015 76%|███████▋  | 382/500 [02:13<00:42,  2.77it/s]#015 77%|███████▋  | 383/500 [02:13<00:41,  2.81it/s]#015 77%|███████▋  | 384/500 [02:13<00:40,  2.84it/s]#015 77%|███████▋  | 385/500 [02:14<00:40,  2.82it/s]#015 77%|███████▋  | 386/500 [02:14<00:39,  2.86it/s]#015 77%|███████▋  | 387/500 [02:14<00:39,  2.86it/s]#015 78%|███████▊  | 388/500 [02:15<00:39,  2.82it/s]#015 78%|███████▊  | 389/500 [02:15<00:40,  2.74it/s]#015 78%|███████▊  | 390/500 [02:16<00:39,  2.79it/s]#015 78%|███████▊  | 391/500 [02:16<00:38,  2.84it/s]#015 78%|███████▊  | 392/500 [02:16<00:37,  2.88it/s]#015 79%|███████▊  | 393/500 [02:17<00:36,  2.89it/s]#015 79%|███████▉  | 394/500 [02:17<00:36,  2.91it/s]#015 79%|███████▉  | 395/500 [02:17<00:35,  2.92it/s]#015 79%|███████▉  | 396/500 [02:18<00:35,  2.93it/s]#015 79%|███████▉  | 397/500 [02:18<00:35,  2.92it/s]#015 80%|███████▉  | 398/500 [02:18<00:34,  2.92it/s]#015 80%|███████▉  | 399/500 [02:19<00:34,  2.94it/s]#015 80%|████████  | 400/500 [02:19<00:34,  2.93it/s]#015 80%|████████  | 401/500 [02:19<00:33,  2.95it/s]#015 80%|████████  | 402/500 [02:20<00:33,  2.94it/s]#015 81%|████████  | 403/500 [02:20<00:33,  2.93it/s]#015 81%|████████  | 404/500 [02:20<00:32,  2.95it/s]#015 81%|████████  | 405/500 [02:21<00:32,  2.92it/s]#015 81%|████████  | 406/500 [02:21<00:32,  2.94it/s]#015 81%|████████▏ | 407/500 [02:21<00:31,  2.93it/s]#015 82%|████████▏ | 408/500 [02:22<00:31,  2.93it/s]#015 82%|████████▏ | 409/500 [02:22<00:31,  2.93it/s]#015 82%|████████▏ | 410/500 [02:22<00:31,  2.82it/s]#015 82%|████████▏ | 411/500 [02:23<00:31,  2.82it/s]#015 82%|████████▏ | 412/500 [02:23<00:30,  2.85it/s]#015 83%|████████▎ | 413/500 [02:23<00:30,  2.87it/s]#015 83%|████████▎ | 414/500 [02:24<00:30,  2.86it/s]#015 83%|████████▎ | 415/500 [02:24<00:29,  2.86it/s]#015 83%|████████▎ | 416/500 [02:25<00:29,  2.86it/s]#015 83%|████████▎ | 417/500 [02:25<00:28,  2.87it/s]#015 84%|████████▎ | 418/500 [02:25<00:28,  2.87it/s]#015 84%|████████▍ | 419/500 [02:26<00:28,  2.85it/s]#015 84%|████████▍ | 420/500 [02:26<00:28,  2.84it/s]#015 84%|████████▍ | 421/500 [02:26<00:27,  2.84it/s]#015 84%|████████▍ | 422/500 [02:27<00:27,  2.84it/s]#015 85%|████████▍ | 423/500 [02:27<00:27,  2.85it/s]#015 85%|████████▍ | 424/500 [02:27<00:26,  2.87it/s]#015 85%|████████▌ | 425/500 [02:28<00:26,  2.87it/s]#015 85%|████████▌ | 426/500 [02:28<00:25,  2.85it/s]#015 85%|████████▌ | 427/500 [02:28<00:25,  2.84it/s]#015 86%|████████▌ | 428/500 [02:29<00:25,  2.83it/s]#015 86%|████████▌ | 429/500 [02:29<00:25,  2.82it/s]#015 86%|████████▌ | 430/500 [02:29<00:24,  2.85it/s]#015 86%|████████▌ | 431/500 [02:30<00:24,  2.84it/s]#015 86%|████████▋ | 432/500 [02:30<00:23,  2.85it/s]#015 87%|████████▋ | 433/500 [02:30<00:23,  2.84it/s]#015 87%|████████▋ | 434/500 [02:31<00:23,  2.84it/s]#015 87%|████████▋ | 435/500 [02:31<00:23,  2.76it/s]#015 87%|████████▋ | 436/500 [02:32<00:22,  2.79it/s]#015 87%|████████▋ | 437/500 [02:32<00:22,  2.82it/s]#015 88%|████████▊ | 438/500 [02:32<00:21,  2.82it/s]#015 88%|████████▊ | 439/500 [02:33<00:21,  2.82it/s]#015 88%|████████▊ | 440/500 [02:33<00:21,  2.83it/s]#015 88%|████████▊ | 441/500 [02:33<00:20,  2.85it/s]#015 88%|████████▊ | 442/500 [02:34<00:20,  2.84it/s]#015 89%|████████▊ | 443/500 [02:34<00:20,  2.84it/s]#015 89%|████████▉ | 444/500 [02:34<00:19,  2.83it/s]#015 89%|████████▉ | 445/500 [02:35<00:19,  2.87it/s]#015 89%|████████▉ | 446/500 [02:35<00:18,  2.86it/s]#015 89%|████████▉ | 447/500 [02:35<00:18,  2.85it/s]#015 90%|████████▉ | 448/500 [02:36<00:18,  2.84it/s]#015 90%|████████▉ | 449/500 [02:36<00:17,  2.85it/s]#015 90%|█████████ | 450/500 [02:36<00:17,  2.87it/s]#015 90%|█████████ | 451/500 [02:37<00:17,  2.85it/s]#015 90%|█████████ | 452/500 [02:37<00:17,  2.81it/s]#015 91%|█████████ | 453/500 [02:38<00:16,  2.81it/s]#015 91%|█████████ | 454/500 [02:38<00:16,  2.81it/s]#015 91%|█████████ | 455/500 [02:38<00:15,  2.83it/s]#015 91%|█████████ | 456/500 [02:39<00:15,  2.83it/s]#015 91%|█████████▏| 457/500 [02:39<00:15,  2.82it/s]#015 92%|█████████▏| 458/500 [02:39<00:15,  2.80it/s]#015 92%|█████████▏| 459/500 [02:40<00:14,  2.82it/s]#015 92%|█████████▏| 460/500 [02:40<00:14,  2.84it/s]#015 92%|█████████▏| 461/500 [02:40<00:13,  2.87it/s]#015 92%|█████████▏| 462/500 [02:41<00:13,  2.85it/s]#015 93%|█████████▎| 463/500 [02:41<00:13,  2.83it/s]#015 93%|█████████▎| 464/500 [02:41<00:12,  2.82it/s]#015 93%|█████████▎| 465/500 [02:42<00:12,  2.81it/s]#015 93%|█████████▎| 466/500 [02:42<00:12,  2.80it/s]#015 93%|█████████▎| 467/500 [02:43<00:11,  2.80it/s]#015 94%|█████████▎| 468/500 [02:43<00:11,  2.82it/s]#015 94%|█████████▍| 469/500 [02:43<00:10,  2.84it/s]#015 94%|█████████▍| 470/500 [02:44<00:10,  2.88it/s]#015 94%|█████████▍| 471/500 [02:44<00:10,  2.88it/s]#015 94%|█████████▍| 472/500 [02:44<00:09,  2.91it/s]#015 95%|█████████▍| 473/500 [02:45<00:09,  2.90it/s]#015 95%|█████████▍| 474/500 [02:45<00:08,  2.90it/s]#015 95%|█████████▌| 475/500 [02:45<00:08,  2.90it/s]#015 95%|█████████▌| 476/500 [02:46<00:08,  2.89it/s]#015 95%|█████████▌| 477/500 [02:46<00:07,  2.91it/s]#015 96%|█████████▌| 478/500 [02:46<00:07,  2.92it/s]#015 96%|█████████▌| 479/500 [02:47<00:07,  2.92it/s]#015 96%|█████████▌| 480/500 [02:47<00:06,  2.93it/s]#015 96%|█████████▌| 481/500 [02:47<00:06,  2.95it/s]#015 96%|█████████▋| 482/500 [02:48<00:06,  2.93it/s]#015 97%|█████████▋| 483/500 [02:48<00:05,  2.92it/s]#015 97%|█████████▋| 484/500 [02:48<00:05,  2.93it/s]#015 97%|█████████▋| 485/500 [02:49<00:05,  2.92it/s]#015 97%|█████████▋| 486/500 [02:49<00:04,  2.91it/s]#015 97%|█████████▋| 487/500 [02:49<00:04,  2.91it/s]#015 98%|█████████▊| 488/500 [02:50<00:04,  2.92it/s]#015 98%|█████████▊| 489/500 [02:50<00:03,  2.91it/s]#015 98%|█████████▊| 490/500 [02:50<00:03,  2.92it/s]#015 98%|█████████▊| 491/500 [02:51<00:03,  2.92it/s]#015 98%|█████████▊| 492/500 [02:51<00:02,  2.92it/s]#015 99%|█████████▊| 493/500 [02:51<00:02,  2.92it/s]#015 99%|█████████▉| 494/500 [02:52<00:02,  2.91it/s]#015 99%|█████████▉| 495/500 [02:52<00:01,  2.91it/s]#015 99%|█████████▉| 496/500 [02:52<00:01,  2.92it/s]#015 99%|█████████▉| 497/500 [02:53<00:01,  2.93it/s]#015100%|█████████▉| 498/500 [02:53<00:00,  2.92it/s]#015100%|█████████▉| 499/500 [02:53<00:00,  2.93it/s]#015100%|██████████| 500/500 [02:54<00:00,  2.89it/s]#015                                                 #015#015100%|██████████| 500/500 [02:54<00:00,  2.89it/s]\n",
      "#015  0%|          | 0/32 [00:00<?, ?it/s]#033[A\n",
      "#015  6%|▋         | 2/32 [00:00<00:03,  8.49it/s]#033[A\n",
      "#015  9%|▉         | 3/32 [00:00<00:04,  6.51it/s]#033[A\n",
      "#015 12%|█▎        | 4/32 [00:00<00:04,  5.60it/s]#033[A\n",
      "#015 16%|█▌        | 5/32 [00:00<00:05,  5.11it/s]#033[A\n",
      "#015 19%|█▉        | 6/32 [00:01<00:05,  4.81it/s]#033[A\n",
      "#015 22%|██▏       | 7/32 [00:01<00:05,  4.62it/s]#033[A\n",
      "#015 25%|██▌       | 8/32 [00:01<00:05,  4.49it/s]#033[A\n",
      "#015 28%|██▊       | 9/32 [00:01<00:05,  4.41it/s]#033[A\n",
      "#015 31%|███▏      | 10/32 [00:02<00:05,  4.35it/s]#033[A\n",
      "#015 34%|███▍      | 11/32 [00:02<00:04,  4.31it/s]#033[A\n",
      "#015 38%|███▊      | 12/32 [00:02<00:04,  4.29it/s]#033[A\n",
      "#015 41%|████      | 13/32 [00:02<00:04,  4.27it/s]#033[A\n",
      "#015 44%|████▍     | 14/32 [00:03<00:04,  4.25it/s]#033[A\n",
      "#015 47%|████▋     | 15/32 [00:03<00:04,  4.25it/s]#033[A\n",
      "#015 50%|█████     | 16/32 [00:03<00:03,  4.24it/s]#033[A\n",
      "#015 53%|█████▎    | 17/32 [00:03<00:03,  4.24it/s]#033[A\n",
      "#015 56%|█████▋    | 18/32 [00:04<00:03,  4.23it/s]#033[A\n",
      "#015 59%|█████▉    | 19/32 [00:04<00:03,  4.23it/s]#033[A\n",
      "#015 62%|██████▎   | 20/32 [00:04<00:02,  4.23it/s]#033[A\n",
      "#015 66%|██████▌   | 21/32 [00:04<00:02,  4.23it/s]#033[A\n",
      "#015 69%|██████▉   | 22/32 [00:04<00:02,  4.23it/s]#033[A\n",
      "#015 72%|███████▏  | 23/32 [00:05<00:02,  4.23it/s]#033[A\n",
      "#015 75%|███████▌  | 24/32 [00:05<00:01,  4.23it/s]#033[A\n",
      "#015 78%|███████▊  | 25/32 [00:05<00:01,  4.23it/s]#033[A\n",
      "#015 81%|████████▏ | 26/32 [00:05<00:01,  4.23it/s]#033[A\n",
      "#015 84%|████████▍ | 27/32 [00:06<00:01,  4.23it/s]#033[A\n",
      "#015 88%|████████▊ | 28/32 [00:06<00:00,  4.23it/s]#033[A\n",
      "#015 91%|█████████ | 29/32 [00:06<00:00,  4.23it/s]#033[A\n",
      "#015 94%|█████████▍| 30/32 [00:06<00:00,  4.23it/s]#033[A\n",
      "#015 97%|█████████▋| 31/32 [00:07<00:00,  4.23it/s]#033[A#015                                                 #015\n",
      "#015                                               #015#033[A#015100%|██████████| 500/500 [03:01<00:00,  2.89it/s]\n",
      "#015100%|██████████| 32/32 [00:07<00:00,  4.23it/s]#033[A\n",
      "#015                                               #033[A#015                                                 #015#015100%|██████████| 500/500 [03:03<00:00,  2.89it/s]#015100%|██████████| 500/500 [03:03<00:00,  2.73it/s]\n",
      "#015  0%|          | 0/32 [00:00<?, ?it/s]#015  6%|▋         | 2/32 [00:00<00:03,  8.48it/s]#015  9%|▉         | 3/32 [00:00<00:04,  6.52it/s]#015 12%|█▎        | 4/32 [00:00<00:04,  5.61it/s]#015 16%|█▌        | 5/32 [00:00<00:05,  5.11it/s]#015 19%|█▉        | 6/32 [00:01<00:05,  4.80it/s]#015 22%|██▏       | 7/32 [00:01<00:05,  4.61it/s]#015 25%|██▌       | 8/32 [00:01<00:05,  4.49it/s]#015 28%|██▊       | 9/32 [00:01<00:05,  4.40it/s]#015 31%|███▏      | 10/32 [00:02<00:05,  4.35it/s]#015 34%|███▍      | 11/32 [00:02<00:04,  4.31it/s]#015 38%|███▊      | 12/32 [00:02<00:04,  4.29it/s]#015 41%|████      | 13/32 [00:02<00:04,  4.27it/s]#015 44%|████▍     | 14/32 [00:03<00:04,  4.26it/s]#015 47%|████▋     | 15/32 [00:03<00:04,  4.25it/s]#015 50%|█████     | 16/32 [00:03<00:03,  4.24it/s]#015 53%|█████▎    | 17/32 [00:03<00:03,  4.23it/s]#015 56%|█████▋    | 18/32 [00:04<00:03,  4.23it/s]#015 59%|█████▉    | 19/32 [00:04<00:03,  4.23it/s]#015 62%|██████▎   | 20/32 [00:04<00:02,  4.23it/s]#015 66%|██████▌   | 21/32 [00:04<00:02,  4.23it/s]#015 69%|██████▉   | 22/32 [00:04<00:02,  4.22it/s]#015 72%|███████▏  | 23/32 [00:05<00:02,  4.22it/s]#015 75%|███████▌  | 24/32 [00:05<00:01,  4.22it/s]#015 78%|███████▊  | 25/32 [00:05<00:01,  4.22it/s]#015 81%|████████▏ | 26/32 [00:05<00:01,  4.22it/s]#015 84%|████████▍ | 27/32 [00:06<00:01,  4.22it/s]#015 88%|████████▊ | 28/32 [00:06<00:00,  4.22it/s]#015 91%|█████████ | 29/32 [00:06<00:00,  4.22it/s]#015 94%|█████████▍| 30/32 [00:06<00:00,  4.22it/s]#015 97%|█████████▋| 31/32 [00:07<00:00,  4.22it/s]#015100%|██████████| 32/32 [00:07<00:00,  4.46it/s]\n",
      "\n",
      "2022-09-16 01:41:39 Uploading - Uploading generated training model\n",
      "2022-09-16 01:43:12 Completed - Training job completed\n",
      "Training seconds: 529\n",
      "Billable seconds: 529\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how a model is trained and deployed with Amazon SageMaker:\n",
    "![](../imgs/sagemaker-platform.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g5.4xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.8558972477912903}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\": \"I have to work hard to become better.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlops-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "620471c4a35ea1d37dbb0b9997e7291b4a9dc1c0e683a9b116d994aa523af298"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
